{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TEikunsK_dgG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "b7a7141b-ede4-4850-919e-0a1eb3348dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhassaan-solves\u001b[0m (\u001b[33mhassaan-solves-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_190809-1ispuwew</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hassaan-solves-student/huggingface/runs/1ispuwew' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hassaan-solves-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hassaan-solves-student/huggingface' target=\"_blank\">https://wandb.ai/hassaan-solves-student/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hassaan-solves-student/huggingface/runs/1ispuwew' target=\"_blank\">https://wandb.ai/hassaan-solves-student/huggingface/runs/1ispuwew</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 01:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:13]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2699139416217804, 'eval_runtime': 14.2186, 'eval_samples_per_second': 70.33, 'eval_steps_per_second': 8.791, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets torch --quiet\n",
        "\n",
        "# Step 1: Load IMDb dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('imdb')\n",
        "train_dataset = dataset['train'].shuffle(seed=42).select(range(2000))  # small subset for fast training\n",
        "test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# Step 2: Tokenize using DistilBERT\n",
        "from transformers import DistilBertTokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_test.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Step 3: Load pre-trained model for classification\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', num_labels=2\n",
        ")\n",
        "\n",
        "# Step 4: Define training arguments (minimal for compatibility)\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8\n",
        ")\n",
        "\n",
        "# Step 5: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test\n",
        ")\n",
        "\n",
        "# Step 6: Train model\n",
        "trainer.train()\n",
        "\n",
        "# Step 7: Evaluate\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/sentiment_model\")\n",
        "tokenizer.save_pretrained(\"/content/sentiment_model\")\n",
        "!ls /content/sentiment_model\n"
      ],
      "metadata": {
        "id": "0le4QBevipFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8f2f9c-a090-4760-c3d6-cfd9588790a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t   special_tokens_map.json  vocab.txt\n",
            "model.safetensors  tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r sentiment_model.zip /content/sentiment_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEsxRXZ7j7U4",
        "outputId": "9e79f7fc-6020-4fa7-afc1-164de60c77b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/sentiment_model/ (stored 0%)\n",
            "updating: content/sentiment_model/config.json (deflated 45%)\n",
            "updating: content/sentiment_model/tokenizer_config.json (deflated 75%)\n",
            "updating: content/sentiment_model/model.safetensors (deflated 8%)\n",
            "updating: content/sentiment_model/vocab.txt (deflated 53%)\n",
            "updating: content/sentiment_model/special_tokens_map.json (deflated 42%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sentiment_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EUExJ4LpkEww",
        "outputId": "6cd232c7-1afd-4425-c8e0-d4f5da10d8e5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f867128c-9996-4da6-b05c-c7ac3f01255d\", \"sentiment_model.zip\", 246889438)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"/content/sentiment_model\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"/content/sentiment_model\")\n",
        "\n",
        "# Predict function\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        prediction = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][prediction].item()\n",
        "    return {\"prediction\": prediction, \"confidence\": confidence}\n",
        "\n",
        "# Example predictions\n",
        "print(predict_sentiment(\"I absolutely loved this movie!\"))\n",
        "print(predict_sentiment(\"It was boring and disappointing.\"))\n"
      ],
      "metadata": {
        "id": "GlrNchzKkIxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd14b88-445d-4033-bcc1-3965aa7fff73"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prediction': 1, 'confidence': 0.9720109701156616}\n",
            "{'prediction': 0, 'confidence': 0.9745967984199524}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"/content/sentiment_model\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"/content/sentiment_model\")\n",
        "\n",
        "# Prediction function\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        prediction = \"Positive\" if torch.argmax(probs) == 1 else \"Negative\"\n",
        "        confidence = probs[0][torch.argmax(probs)].item()\n",
        "    return f\"{prediction} ({confidence:.2f} confidence)\"\n",
        "\n",
        "# Gradio UI\n",
        "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\", title=\"Sentiment Classifier\").launch()\n"
      ],
      "metadata": {
        "id": "n9mxJaUjsYBA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "ac7e51d6-f6f6-47ce-b48c-fb89df161933"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f99fed3b8b7ca81a99.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f99fed3b8b7ca81a99.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}